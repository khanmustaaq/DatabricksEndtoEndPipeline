
# ğŸ“Š Azure Databricks End-to-End Lakehouse Project ğŸš€

This project implements a production-grade ETL pipeline using **Azure Databricks** following the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold). It uses:

- ğŸ”„ **Auto Loader** for streaming & incremental data ingestion  
- ğŸ”¥ **Delta Live Tables (DLT)** for declarative transformations  
- ğŸ¢ **Unity Catalog** for governance  
- ğŸ§  **SCD Type 1** logic for warehouse dimensions  
- ğŸ’¾ **ADLS Gen2** for layered cloud storage  

---

## ğŸ—ï¸ Architecture Overview

```plaintext
+---------------------+
|   Source Parquet    |
+---------------------+
          â†“
     Bronze Layer (Raw Ingest via Auto Loader)
          â†“
     Silver Layer (Cleaned DataFrames with schema)
          â†“
     Gold Layer (Fact & Dim tables with SCD logic)
````

---

## ğŸ”¹ Bronze Layer â€” Raw Ingestion

âœ… Ingested source data using **Auto Loader**
âœ… Parameterized ingestion notebook (`file_name` input widget)
âœ… Streaming Delta Tables with checkpointing for **Exactly Once**

ğŸ“· **Auto Loader Notebook:**

![Auto Loader Config](screenshots/auto_loader_notebook.png)

---

## ğŸ”¸ Silver Layer â€” Cleaned Data

âœ… PySpark transformations
âœ… Dropped duplicates, selected columns, schema validation
âœ… Tables registered to **Unity Catalog**

ğŸ“· **Silver Layer Table in Catalog:**

![Silver Output](screenshots/silver_output_catalog.png)

---

## ğŸŸ¡ Gold Layer â€” Business Logic & DWH

âœ… Created **FactOrders** and **DimCustomers**
âœ… Applied **SCD Type 1** (overwrite on match, no history)
âœ… Used `merge()` for UPSERT logic
âœ… Created surrogate keys (`monotonically_increasing_id()` + offset)

ğŸ“· **SCD Logic using Delta + PySpark:**

![DLT SCD Screenshot](screenshots/dlt_scd.png)

---

## âœ… DLT Table Expectations

Used `@dlt.expect_all_or_drop()` to enforce constraints on streaming tables:

```python
my_rules = {
    "rule1": "product_id IS NOT NULL",
    "rule2": "product_name IS NOT NULL"
}

@dlt.expect_all_or_drop(my_rules)
@dlt.table()
def DimProducts_stage():
    return spark.readStream.table("databricks_catalog.silver.products_silver")
```

---

## ğŸ“ ADLS Gen2 Layered Storage

Used **ABFSS URI** format to access containers:

```python
.option("path", "abfss://gold@datalakemushtaaqe2e.dfs.core.windows.net/FactOrders")
```

Separate containers used for:

* `source/` (raw data)
* `bronze/`, `silver/`, `gold/` (ETL stages)
* `metastore/` (Unity Catalog backing store)

---

## ğŸ§  Tech Stack

| Tool              | Usage                         |
| ----------------- | ----------------------------- |
| Azure Databricks  | Compute, Notebooks, Jobs      |
| Delta Lake        | Storage Format + ACID         |
| Unity Catalog     | Centralized Schema Governance |
| Azure ADLS Gen2   | Cloud Object Storage          |
| PySpark / SQL     | Transformations               |
| Delta Live Tables | Declarative Streaming & DWH   |
| RocksDB           | Stream State Management       |

---

## ğŸ’» Project Structure

```
AzureLakehouseProject/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ bronze_ingestion.ipynb
â”‚   â”œâ”€â”€ silver_customers.ipynb
â”‚   â”œâ”€â”€ silver_orders.ipynb
â”‚   â”œâ”€â”€ gold_dim_customers.ipynb
â”‚   â””â”€â”€ gold_fact_orders.ipynb
â”‚
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ auto_loader_notebook.png
â”‚   â”œâ”€â”€ silver_output_catalog.png
â”‚   â”œâ”€â”€ dlt_scd.png
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸš€ How to Run This Project

1. ğŸ”§ Set up Azure ADLS Gen2 with containers:

   * `source/`, `bronze/`, `silver/`, `gold/`
2. ğŸ” Set up **Unity Catalog + External Locations**
3. ğŸ“¥ Upload source Parquet files to the `source/` container
4. ğŸ”„ Run the **Bronze Auto Loader notebook**
5. ğŸ§¹ Run **Silver notebooks** to transform & clean data
6. ğŸ›ï¸ Run **Gold notebooks** to build fact/dim tables
7. ğŸ” Automate via **Databricks Workflows** (parameterized)

---

## ğŸ§© Extra Notes

* Used **RocksDB** under the hood for structured streaming state store
* Streaming tables and views used inside DLT
* Handles **Initial + Incremental** loading cleanly
* Parameterized ingestion supports looping through datasets using jobs

---


## ğŸ‘¨â€ğŸ’» Author

**Mustaaq Ahmed Khan**
Databricks | Azure | F1 Strategy Enthusiast
ğŸ”— [LinkedIn](https://www.linkedin.com/in/mustaaq-ahmed-khan-1b7189245/) | ğŸ§  Passionate about ETL + Race Data + Real-time Systems

---

â­ Star this repo if you found it helpful!



